# Meta's SAM 3 segmentation model blurs the boundary between language and vision
- ğŸ“… æ¥æº: The Decoder
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2025-11-22 01:53:26
- ğŸ”— [åŸæ–‡é“¾æ¥](https://the-decoder.com/metas-sam-3-segmentation-model-blurs-the-boundary-between-language-and-vision/)

<p><img alt="" class="attachment-full size-full wp-post-image" height="659" src="https://the-decoder.com/wp-content/uploads/2025/11/meta_sam_3.png" style="height: auto; margin-bottom: 10px;" width="1183" /></p>
<p>        Meta releases the third generation of its "Segment Anything Model." Unlike standard models limited to fixed categories, SAM 3 uses an open vocabulary to understand both images and videos. The system relies on a new training method combining human and AI annotators.</p>
<p>The article <a href="https://the-decoder.com/metas-sam-3-segmentation-model-blurs-the-boundary-between-language-and-vision/">Meta&#039;s SAM 3 segmentation model blurs the boundary between language and vision</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p>