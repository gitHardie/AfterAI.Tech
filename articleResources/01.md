# Confident user prompts make LLMs more likely to hallucinate
- ğŸ“… æ¥æº: The Decoder
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2025-05-12 00:01:19
- ğŸ”— [åŸæ–‡é“¾æ¥](https://the-decoder.com/confident-user-prompts-make-llms-more-likely-to-hallucinate/)

<p><img alt="Even small changes to the prompt can have a major impact on the quality of facts: A new benchmark shows how susceptible language models are to brevity statements and exaggerated user inflection." class="attachment-full size-full wp-post-image" height="1024" src="https://the-decoder.com/wp-content/uploads/2025/05/phare_benchmark_hallucinations.png" style="height: auto; margin-bottom: 10px;" width="1536" /></p>
<p>        Many language models are more likely to generate incorrect information when users request concise answers, according to a new benchmark study.</p>
<p>The article <a href="https://the-decoder.com/confident-user-prompts-make-llms-more-likely-to-hallucinate/">Confident user prompts make LLMs more likely to hallucinate</a> appeared first on <a href="https://the-decoder.com">THE DECODER</a>.</p>