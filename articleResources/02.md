# Machine Learning System Monitors Patient Pain During Surgery
- ğŸ“… æ¥æº: IEEE Spectrum AI
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2026-01-13 00:52:21
- ğŸ”— [åŸæ–‡é“¾æ¥](https://spectrum.ieee.org/machine-learning-measure-pain-surgery)

<img src="https://spectrum.ieee.org/media-library/illustration-of-a-womans-face-where-one-half-is-intact-and-the-other-is-segmented-into-several-triangular-pieces-a-web-of-red.jpg?id=62674391&amp;width=1200&amp;height=800&amp;coordinates=62%2C0%2C63%2C0" /><br /><br /><p><em>This article is part of our exclusive <a href="https://spectrum.ieee.org/collections/journal-watch/" target="_self">IEEE Journal Watch series</a> in partnership with <a href="https://spectrum.ieee.org/tag/ieee-xplore" target="_self">IEEE Xplore</a>.</em></p><p>In the operating room, patients undergoing procedures with local anesthesia, while still conscious, may have difficulty expressing their levels of pain. Some, such as infants or people with dementia, may not be able to communicate these feelings at all. In the search for a better way to monitor patientsâ€™ pain, a team of researchers has developed a contactless method which analyzes a combination of patientsâ€™ heart rate data and facial expressions to estimate the pain theyâ€™re feeling. The approach is described in a <a href="https://ieeexplore.ieee.org/document/11249745" rel="noopener noreferrer" target="_blank">study</a> published 14 November in the <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8782705" rel="noopener noreferrer" target="_blank"><em>IEEE Open Journal of Engineering in Medicine and Biology</em></a>.<strong></strong></p><p><a href="https://science-careers.htwk-leipzig.de/en/mainnavigation/female-scientists-network/portraits/alumnae#c213060" target="_blank">Bianca Reichard</a>, a researcher at the Institute for Applied Informatics in Leipzig, Germany, notes that camera-based pain monitoring sidesteps the need for patients to wear sensors with wires, such as ECG electrodes and blood pressure cuffs, which could interfere with the delivery of medical care.</p><p>To create their contactless approach, the researchers created a machine learning algorithm capable of analyzing aspects of pain that can be detected visually by a camera. First, the algorithm analyzes the nuances of a personâ€™s facial expressions to estimate their pain levels. </p><p>The system also uses heart rate data via a technique called <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9267568/" target="_blank">remote photoplethysmogram</a> (rPPG), which involves shining a light on a personâ€™s skin. The amount of light reflected back can be used to detect changes in blood volume within their vessels. The researchers initially considered 15 different heart rate variability parameters measured by rPPG to include in their model, and selected the top seven that are statistically most relevant to pain prediction, such as heart rate maximums, minimums, and intervals. </p><h2>Pain Prediction Model Training Datasets</h2><p>The team used two different datasets to train and test their pain prediction model. One is a well-established and widely used database used to measure pain called the <a href="https://www.nit.ovgu.de/nit/en/BioVid-p-1358.html" target="_blank">BioVid Heat Pain Database</a>. Researchers <a href="https://www.researchgate.net/publication/259990721_Towards_Pain_Monitoring_Facial_Expression_Head_Pose_a_new_Database_an_Automatic_System_and_Remaining_Challenges" target="_blank">created this dataset in 2013</a> through experiments in which thermodes induced incremental, measurable temperature increases on individualsâ€™ skin. The researchers then captured the participants physical responses to the corresponding pain that they felt.</p><p><strong></strong>The second dataset was developed by the researchers for this new work. 29 patients undergoing heart procedures involving insertion of a catheter were surveyed about their pain levels at 5 minute intervals.<strong></strong></p><p>Importantly, most other pain prediction algorithms have been trained using very short video clips, but Reichard and her team specifically used longer training videos (ranging from 30 minutes to 3 hours) of realistic surgery scenarios to train their model. For instance, the training videos used may have included scenarios where lighting may not be ideal, or the patientâ€™s face may be partially obscured from the camera at times. â€œThis reflects a more realistic clinical situation compared to laboratory data sets,â€ Reichard explains. </p><p>Tests of their model show that it has a pain prediction accuracy of about 45 percent. <span>Reichard says she is surprised that the model is so accurate, </span><span>given the number of disruptions that occurred throughout the raw video footage, such a patient moving on the operating table or changes in the camera angle.  </span>While many previously developed pain prediction models can achieve higher accuracies, those were trained using short video clips that are â€œidealâ€ with no visual obstructions. Instead, this research team used less than idealâ€”but more realisticâ€”video footage to train their model.</p><p>Whatâ€™s more, Reichard notes that the team used a fairly simple statistical machine learning model. â€œUsing more complex approaches, for example based on neural networks, would most likely further improve performance,â€ she says.</p><p>Reichard says she finds this type of researchâ€”which could support both patients and medical staffâ€”meaningful, and is planning on developing similar contactless systems for measuring patientsâ€™ vital sign using radar in medical settings, in future work. </p>