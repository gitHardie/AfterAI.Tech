# Thermodynamic Computing Slashes AI Image Energy Use
- ğŸ“… æ¥æº: IEEE Spectrum AI
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2026-01-28 04:38:17
- ğŸ”— [åŸæ–‡é“¾æ¥](https://spectrum.ieee.org/thermodynamic-computing-for-ai)

<img src="https://spectrum.ieee.org/media-library/abstract-graphic-of-1-s-and-2-s-against-a-pixelated-background.jpg?id=63302559&amp;width=1200&amp;height=600&amp;coordinates=0%2C250%2C0%2C250" /><br /><br /><p><a href="https://spectrum.ieee.org/what-is-generative-ai" target="_self">Generative AI</a> tools such as <a href="https://spectrum.ieee.org/openai-dall-e-2" target="_self">DALL-E</a>, <a href="https://spectrum.ieee.org/these-ai-tools-generate-breathtaking-art-and-controversy" target="_self">Midjourney</a>, and <a href="https://spectrum.ieee.org/ai-art-generator" target="_self">Stable Diffusion</a> create photorealistic images. However, they <a href="https://spectrum.ieee.org/ai-energy-consumption" target="_self">burn lavish amounts of energy</a>. Now a pair of studies finds that so-called <a href="https://spectrum.ieee.org/thermodynamic-computing-normal-computing" target="_self">thermodynamic computing</a> might generate images using one ten billionth the energy.</p><p>At the heart of many AI image generators are machine learning algorithms known as <a href="https://spectrum.ieee.org/best-ai-image-generator" target="_self">diffusion models</a>. Programmers feed the models large sets of images to which they gradually add noise until they resemble the static on an out-of-tune analog television. They then train <a href="https://spectrum.ieee.org/deep-neural-network" target="_self">neural networks</a> to reverse this process, enabling diffusion models to generate entirely new images given prompting.<strong></strong></p><p>However, the AI digital computations that add noise and then conjure pictures from the static are energy-hungry. Now a new technique involving thermodynamic computing might generate images â€œwith a much lower energy cost than current digital hardware can,â€ says <a href="https://foundry.lbl.gov/about/staff/stephen-whitelam/" rel="noopener noreferrer" target="_blank">Stephen Whitelam</a>, a staff scientist at Lawrence Berkeley National Laboratory in California.<br /></p><p><strong>Using natureâ€™s noise</strong></p><p>Thermodynamic computing employs physical circuits that changes in response to noise, such as that caused by random thermal fluctuations in the environment, to perform low-energy computations. For instance, a <a href="https://www.nature.com/articles/s41467-025-59011-x" target="_blank">prototype chip</a> from New York cityâ€“based startup Normal Computing consists of eight resonators connected to each other via special couplers. Programmers use the couplers to build a kind of calculator customized for the problem they want to study. Then they pluck the resonators, which introduce noise into the resonator-coupling network, performing the calculation. After the system reaches equilibrium, the programmers can read the solution in the new configuration of the resonators.</p><p>In a 10 January <a href="https://www.nature.com/articles/s41467-025-67958-0" target="_blank">Nature Communications article</a>, Whitelam and a colleague revealed it was possible to create a thermodynamic version of a neural network. This lays the groundwork for image generation via thermodynamic computing.</p><p>Whitelamâ€™s new strategy would give a thermodynamic computer a set of images. The technique would then allow those stored pictures to degrade by letting the natural random interactions between the computerâ€™s components run until the couplings linking these components naturally reach a state of equilibrium. Next, the strategy would compute the probability that a thermodynamic computer with a given state of couplings could reverse the decay process. Then it would adjust the values of these couplings to maximize that probability.</p><p>In simulations run on conventional computers published January 20 in <em><a href="https://doi.org/10.1103/kwyy-1xln">Physical Review Letters</a></em>, Whitelam found this training process can lead to a thermodynamic computer whose settings can generate images of handwritten digits. It could accomplish this without energy-intensive digital neural networks or a noise-generating pseudorandom number generator.</p><p>â€œThis research suggests that itâ€™s possible to make hardware to do certain types of machine learning â€” here, image generation â€” with considerably lower energy cost than we do at present,â€ Whitelam says.</p><p>Whitelam cautions that thermodynamic computers are currently rudimentary when compared with digital neural networks. â€œWe donâ€™t yet know how to design a thermodynamic computer that would be as good at image generation as, say, DALL-E,â€ he says. â€œIt will still be necessary to work out how to build the hardware to do this.â€</p><p>Although he calculates that thermodynamic computers might have a huge advantage over regular computers in terms of energy efficiency, â€œit will be challenging to build a thermodynamic computer that can enjoy all of that advantage. Itâ€™s likely that near-term designs will be something in between that ideal and current digital power levels.â€</p>