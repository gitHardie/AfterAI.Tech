# Proactive Hearing Assistant Filters Through Voices in a Crowd
- ğŸ“… æ¥æº: IEEE Spectrum AI
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2025-12-09 00:00:02
- ğŸ”— [åŸæ–‡é“¾æ¥](https://spectrum.ieee.org/proactive-ai-hearing-devices)

<img src="https://spectrum.ieee.org/media-library/close-up-of-a-small-earbud-taped-to-the-exterior-face-of-a-headphone-speaker.jpg?id=62288925&amp;width=1200&amp;height=800&amp;coordinates=0%2C107%2C0%2C107" /><br /><br /><p><span><em></em></span>Inside a crowded bar, even the best noise-canceling earbuds struggle. They can either shut the whole world out or let everything in, but they canâ€™t do what humans do naturally: focus on the voices that matter while ignoring everything else. A new study from researchers at the University of Washington <a href="https://aclanthology.org/2025.emnlp-main.1289/" target="_blank">proposes a third way</a>â€”a â€œproactive hearing assistantâ€ that automatically figures out who youâ€™re talking to using AI and enhances only their voices in real time, without taps or gestures.</p><p>â€œWe were asking a very simple question,â€ says <a href="https://homes.cs.washington.edu/~gshyam/" target="_blank">Shyam Gollakota</a>, head of the Mobile Intelligence Lab at the University of Washington and coauthor of the study. â€œIf youâ€™re in a bar with a hundred people, how does the AI know who you are talking to?â€</p><p>The teamâ€™s answer blends audio engineering with conversational science. Building on <a href="https://spectrum.ieee.org/noise-cancelling-headphones" target="_blank">previous research by Gollakotaâ€™s lab</a>, the system uses AI trained to detect the subtle turn-taking patterns humans instinctively follow to alternate speaking turns with minimal overlap. That conversational rhythm becomes the cue for identifying who is in the exchange. Voices that donâ€™t follow the pattern are filtered out.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/noise-cancelling-headphones" target="_blank">AI Headphones Create Cones of Silence</a></p><p>The prototype uses microphones in both ears and a<strong> </strong>directional audio filter aimed at the wearerâ€™s mouth to extract the userâ€™s own speech, which acts as an anchor for detecting turn-taking. With that anchor, the system isolates and enhances conversation partners while suppressing everyone else, operating at latencies less than ten millisecondsâ€”fast enough to keep the amplified audio aligned with lip movements.</p><p>â€œThe key insight is intuitive,â€<strong> </strong>Gollakota says. â€œIf Iâ€™m having a conversation with you, we arenâ€™t talking over each other as much as people who are not part of the conversation.â€ The AI identifies voices that alternate naturally with the wearerâ€™s own and ignores those that overlap too often to fit the conversation. The method does not rely on proximity, loudness, direction, or pitch. â€œWe donâ€™t use any sensors beyond audio,â€ he says. â€œYou could be looking away, or someone farther away could be speaking louderâ€”it still works.â€</p><p>The technology could be useful to people who have hearing challenges, as traditional hearing aids amplify all sound and noise alike. â€œIt could be extremely powerful for quality of life,â€ says Gollakota. Proactive hearing assistants with this technology could also help older users who would struggle to manually select speakers to amplify. <strong></strong></p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Headphones with one earbud taped to the exterior face of each speaker resting on a table next to a smart phone." class="rm-shortcode" id="5e3e4" src="https://spectrum.ieee.org/media-library/headphones-with-one-earbud-taped-to-the-exterior-face-of-each-speaker-resting-on-a-table-next-to-a-smart-phone.jpg?id=62288931&amp;width=980" /> <small class="image-media media-caption">To deal with latency issues, the system uses a two-part model that mimics how our brains also process conversation. </small><small class="image-media media-photo-credit">Shyam Gollakota</small></p><h3>A Brain-Inspired Dual Model</h3><p>To feel natural, conversational audio must be processed in under ten milliseconds, but detecting turn-taking patterns requires one to two seconds of context. Reconciling those timescales required a split architecture: a slower model that updates once per second and a faster model that runs every 10 to 12 milliseconds.</p><p>The slower model infers conversational dynamics and generates a â€œconversational embedding.â€ The fast model uses that embedding to extract only the identified partner voices, suppressing all others quickly enough for seamless dialogue. Gollakota compares the process to how the brain separates slower deliberation from quick speech production. â€œThereâ€™s a slower process making sense of the conversation, and a much faster process that responds almost instantaneously,â€ he says.</p><p>Conversational rhythm varies across cultures, so the team trained the system on both English and Mandarin. It generalized to Japanese conversations despite never being trained on Japaneseâ€”evidence, they say, that the model is capturing universal timing cues.</p><p>In controlled tests, the system identified conversation partners with 80 to 92 percent accuracy and had 1.5 to 2.2 percent confusion (meaning the system identified an outside speaker as being part of the conversation by mistake). It improved speech clarity by up to 14.6 dB. <strong></strong></p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span> <small class="image-media media-caption">Listen to the difference the hearing assistant makes when itâ€™s turned on</small> </p><h3>Promise and Boundaries</h3><p>â€œWhat they describe is an interesting and novel direction. But when it comes to real-world applications, many challenges remain,â€ says <a href="https://www.linkedin.com/in/te-won-lee-255b3a15" target="_blank">Te-Won Lee</a>, CEO of AI glasses company <a href="https://www.softeye.ai/" target="_blank">SoftEye</a>, who has recently developed a similar technology for commercial use. Leeâ€™s tech was based on blind source separation, a signal processing technique that tries to sift individual sound sources from a mixture of sounds without knowing what the sources are in advance. <strong></strong></p><p>â€œIn most environments, you donâ€™t get four people neatly taking turns,â€ Lee says. â€œYou get music, unpredictable noise, people interrupting each other. The scenarios described in the research are not the scenarios you encounter in most real-world environments.â€ As soundscapes become more chaotic, performance may degrade.</p><p>Still, he sees a major strength in the prototypeâ€™s very low latency. â€œWhen it comes to deployment in millions of devices, latency has to be extremely low,â€ he says. â€œEven 100 milliseconds is unacceptable. You need something close to ten milliseconds.â€</p><p>Lee also notes that decades of <del></del>blind source separation and speech-enhancement work have yielded algorithms that work across many noise conditions to isolate one desired speaker, usually the device user, from all other sources. â€œReal-world speech enhancement is about separating the desired speech from all other noise,â€ Lee says. â€œThose techniques are more geared toward unpredictable environments.â€ But in earbuds or AR glasses, where the system knows whom the wearer intends to talk to, he says the UW approach â€œcan be very effective if the scenario matches their assumptions.â€</p><h3>Risks, Limitations, and Next Steps</h3><p>The system relies heavily on self-speech, so long silences can confuse it. Overlapping speech and simultaneous turn-changes remain challenging. The method is not suited for passive listening, since it assumes active participation. And because conversational norms vary culturally, additional fine-tuning may be needed.</p><p>Incorrect detection can also amplify the wrong personâ€”a real risk in fast-moving exchanges. Lee adds that unpredictable noise, from music to chaotic soundscapes, remains a major hurdle. â€œThe real world is messy,â€ he says.</p><p>Next, the team plans to incorporate semantic understanding using large language models so that future versions can infer not only who is speaking but who is contributing meaningfully, making hearing assistants more flexible and more humanlike in how they follow conversations.</p>