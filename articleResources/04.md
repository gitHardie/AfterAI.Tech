# AI Is Acing Math Exams Faster Than Scientists Write Them
- ğŸ“… æ¥æº: IEEE Spectrum AI
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2026-02-26 00:00:02
- ğŸ”— [åŸæ–‡é“¾æ¥](https://spectrum.ieee.org/ai-math-benchmarks)

<img src="https://spectrum.ieee.org/media-library/line-graph-demonstrating-how-google-deep-mind-s-aletheia-ai-scores-at-least-5-higher-on-ph-d-math-exercises-than-the-latest-ver.jpg?id=65007034&amp;width=1245&amp;height=700&amp;coordinates=0%2C62%2C0%2C63" /><br /><br /><p><span>Mathematics is often regarded as the ideal domain for measuring AI progress effectively. Mathâ€™s step-by-step logic is easy to track, and its definitive automatically verifiable answers remove any human or subjective factors. But AI systems are improving at such a pace that math </span><a href="https://spectrum.ieee.org/melanie-mitchell" target="_self">benchmarks are struggling to keep up</a><span>.</span></p><p>Way back in November 2024, non-profit research organization Epoch AI quietly released <a href="https://doi.org/10.48550/arXiv.2411.04872" target="_blank">FrontierMath</a>. A standardized, rigorous benchmark, Frontier Math was designed to measure the mathematical reasoning capabilities of the latest AI tools.</p><p>â€œItâ€™s a bunch of really hard math problems,â€ explains <a href="https://epoch.ai/team" target="_blank">Greg Burnham</a>, Epoch AI Senior Researcher. â€œOriginally, it was 300 problems that we now call tiers 1â€“3, but having seen AI capabilities really speed up, there was a feeling that we had to run to stay ahead, so now thereâ€™s a special challenge set of extra carefully constructed problems that we call tier 4.â€</p><p>To a rough approximation, tiers 1â€“4 go from advanced undergraduate through to early postdoc level mathematics. When introduced, state-of-the-art AI models were unable to solve more than 2% of the problems FrontierMath contained. <a href="https://epoch.ai/frontiermath/tiers-1-4" target="_blank">Fast forward to today</a> and the best publicly available AI models, such as GPT-5.2 and Claude Opus 4.6, are solving over 40% of FrontierMathâ€™s 300 tiers 1â€“3 problems, and over 30% of the 50 tier 4 problems.</p><h2>AI takes on PhD level mathematics</h2><p>And this dizzying pace of advancement is showing no signs of abating. For example, just recently <a href="https://deepmind.google/blog/accelerating-mathematical-and-scientific-discovery-with-gemini-deep-think/" target="_blank">Google DeepMind announced</a> that Aletheia, an experimental AI system derived from Gemini Deep Think, <a href="https://doi.org/10.48550/arXiv.2601.23245" rel="noopener noreferrer" target="_blank">achieved publishable PhD level research results</a>. Though obscure mathematicallyâ€”calculating certain structure constants in arithmetic geometry called eigenweightsâ€”the result is significant in terms of AI development.</p><p>â€œTheyâ€™re claiming it was essentially autonomous, meaning a human wasnâ€™t guiding the work, and itâ€™s publishable,â€ Burnham says. â€œItâ€™s definitely at the lower end of the spectrum of work that would get a mathematician excited, but itâ€™s newâ€”itâ€™s something we truly havenâ€™t really seen before.â€</p><p>To place this achievement in context, every FrontierMath problem has a known answer that a human has derived. Though a human could probably have achieved Aletheiaâ€™s result â€œif they sat down and steeled themselves for a week,â€ says Burnham, no human had ever done so.</p><p>Aletheiaâ€™s results and other recent achievements by AI mathematicians point to new, tougher benchmarks being needed to understand AI capabilities, and fast, because existing ones will soon become irrelevant. â€œThere are easier math benchmarks that are already obsolete, several generations of them,â€ says Burnham. â€œFrontierMath will probably saturate [meaning state-of-the-art AI models score 100%] within the next two years; could be faster.â€</p><h2>The First Proof challenge</h2><p>To begin to address this problem, on February 6, a group of 11 highly distinguished mathematicians <a href="https://doi.org/10.48550/arXiv.2602.05192" rel="noopener noreferrer" target="_blank">proposed the First Proof challenge</a>, a set of 10 extremely difficult math questions which arose naturally in the authorsâ€™ research processes, and whose proofs are roughly five pages or less and had not been shared with anyone. <a href="https://1stproof.org/" rel="noopener noreferrer" target="_blank">The First Proof challenge</a> was a preliminary effort to assess the capabilities of AI systems in solving research-level math questions on their own.</p><p>Generating serious buzz in the math community, professional and amateur mathematicians, and teams including OpenAI, all stepped up to the challenge. But by the time the authors <a href="https://codeberg.org/tgkolda/1stproof/src/branch/main/2026-02-batch/FirstProofSolutionsComments.pdf" rel="noopener noreferrer" target="_blank">posted the proofs</a> on February 14, no one had submitted correct solutions to all 10 problems.</p><p>In fact, far from it. The authors themselves only solved two of the 10 problems using Gemini 3.0 Deep Think and ChatGPT 5.2 Pro. And most outside submissions fared little better, apart from OpenAI and a small Aletheia team at Google DeepMind. With â€œlimited human supervisionâ€ OpenAIâ€™s most advanced internal AI system <a href="https://openai.com/index/first-proof-submissions/" rel="noopener noreferrer" target="_blank">solved five of the 10 problems</a>, with Aletheia achieving similar outcomesâ€”results met with a spectrum of emotions by different members of the mathematics community, from awe to disappointment. The team behind First Proof plans an even tougher <a href="https://1stproof.org/" rel="noopener noreferrer" target="_blank">second round on March 14</a>.</p><h2>A new frontier for AI</h2><p>â€œI think First Proof is terrific: itâ€™s as close as you could realistically get to putting an AI system in the shoes of a mathematician,â€ says Burnham. Though he admires how First Proof tests AIâ€™s mathematical utility for a wide range of mathematics and mathematicians, Epoch AI has its own new approach to testingâ€”<a href="https://epoch.ai/frontiermath/open-problems" rel="noopener noreferrer" target="_blank">FrontierMath: Open Problems</a>. Uniquely, the pilot benchmark consists of 16 open problems (with more to follow) from research mathematics that professional mathematicians have tried and failed to solve. Since Open Problemsâ€™ <a href="https://epochai.substack.com/p/introducing-frontiermath-open-problems" rel="noopener noreferrer" target="_blank">release on January 27</a>, none have been solved by an AI.</p><p>â€œWith Open Problems, weâ€™ve tried to make it more challenging,â€ says Burnham. â€œThe baseline on its own would be publishable, at least in a specialty journal.â€ Whatâ€™s more, each question is designed so that it can be automatically graded. â€œThis is a bit counterintuitive,â€ Burnham adds. â€œNo one knows the answers, but we have a computer program that will be able to judge whether the answer is right or not.â€</p><p>Burnham sees First Proof and Open Problems as being complementary. â€œI would say understanding AI capabilities is a more-the-merrier situation,â€ he adds. â€œAI has gotten to the point where itâ€™s, in some ways, better than most PhD students, so we need to pose problems where the answer would be at least moderately interesting to some human mathematicians, not because AI was doing it, but because itâ€™s mathematics that human mathematicians care about.â€</p>