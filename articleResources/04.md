# Are We Testing AIâ€™s Intelligence the Wrong Way?
- ğŸ“… æ¥æº: IEEE Spectrum AI
- ğŸ•’ å‘å¸ƒæ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰: 2025-12-05 07:30:02
- ğŸ”— [åŸæ–‡é“¾æ¥](https://spectrum.ieee.org/melanie-mitchell)

<img src="https://spectrum.ieee.org/media-library/an-illustration-of-a-scientist-pointing-to-a-meter-on-a-brain-the-background-is-circuitry.jpg?id=62282615&amp;width=1200&amp;height=800&amp;coordinates=62%2C0%2C63%2C0" /><br /><br /><p><span>When people want a clear-eyed take on the state of artificial intelligence and what it all means, they tend to turn to </span><a href="https://melaniemitchell.me/" target="_blank">Melanie Mitchell</a><span>, a computer scientist and a </span><a href="https://santafe.edu/people/profile/melanie-mitchell" target="_blank">professor</a><span> at the Santa Fe Institute. Her 2019 book, <em><a href="https://melaniemitchell.me/aibook/" target="_blank">Artificial Intelligence: A Guide for Thinking Humans</a></em>, helped define the modern conversation about what todayâ€™s AI systems can and canâ€™t do.</span></p> <p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" style="float: left;"> <img alt="A smiling bespectacled woman with shoulder length brown hair." class="rm-shortcode" id="66ed8" src="https://spectrum.ieee.org/media-library/a-smiling-bespectacled-woman-with-shoulder-length-brown-hair.jpg?id=62282625&amp;width=980" /> <small class="image-media media-caption">Melanie Mitchell</small></p><p><span>Today </span><span>at NeurIPS, the yearâ€™s biggest gathering of AI professionals, she gave a </span><a href="https://neurips.cc/virtual/2025/loc/san-diego/invited-talk/109607" target="_blank">keynote</a><span> titled </span><span>â€œOn the Science of â€˜</span><span>Alien Intelligencesâ€™</span><span>: Evaluating Cognitive Capabilities in Babies, Animals, and AI.â€</span><span> </span><span>Ahead of the talk, she spoke with </span><em>IEEE Spectrum</em><span> about </span><span>its</span><span> themes: W</span><span>hy todayâ€™s AI systems should be studied more like nonverbal minds, what developmental and comparative psychology can teach AI researchers, and how better experimental methods could reshape the way we measure machine cognition.</span></p><p><strong>You use the phrase â€œalien intelligencesâ€ for both AI and biological minds like babies and animals. What do you mean by that?</strong></p><p><strong>Melanie Mitchell:</strong> Hopefully you noticed the quotation marks around â€œalien intelligences.â€ Iâ€™m quoting from a paper by [the neural network pioneer] <a href="https://www.salk.edu/scientist/terrence-sejnowski/" target="_blank">Terrence Sejnowski</a> where he talks about ChatGPT as <a href="https://papers.cnl.salk.edu/PDFs/Large%20Language%20Models%20and%20the%20Reverse%20Turing%20Test%202023-4640.pdf" target="_blank">being like a space alien</a> that can communicate with us and seems intelligent. And then thereâ€™s another paper by the developmental psychologist <a href="https://web.stanford.edu/~mcfrank/" target="_blank">Michael Frank</a> who plays on that theme and says, <a href="https://www.nature.com/articles/s44159-023-00211-x" target="_blank">we in developmental psychology study alien intelligences</a>, namely babies. And we have some methods that we think may be helpful in analyzing AI intelligence. So thatâ€™s what Iâ€™m playing on.</p><p><strong>When people talk about evaluating intelligence in AI, what kind of intelligence are they trying to measure? Reasoning or abstraction or world modeling or something else?</strong></p><p><strong>Mitchell:</strong> All of the above. People mean different things when they use the word intelligence, and intelligence itself has all these different dimensions, as you say. So, I used the term cognitive capabilities, which is a little bit more specific. Iâ€™m looking at how different cognitive capabilities are evaluated in developmental and comparative psychology and trying to apply some principles from those fields to AI.</p><h2>Current Challenges in Evaluating AI Cognition</h2><p><strong>You say that the field of AI lacks good experimental protocols for evaluating cognition. What does AI evaluation look like today?</strong></p><p><strong>Mitchell:</strong> The typical way to evaluate an AI system is to have some set of <a href="https://spectrum.ieee.org/tag/benchmarks" target="_blank">benchmarks</a>, and to run your system on those benchmark tasks and report the accuracy. But often it turns out that even though these AI systems we have now are just killing it on benchmarks, theyâ€™re surpassing humans, that performance doesnâ€™t often translate to performance in the real world. If an AI system aces the bar exam, that doesnâ€™t mean itâ€™s going to be a good lawyer in the real world. Often the machines are doing well on those particular questions but canâ€™t generalize very well. Also, tests that are designed to assess humans make assumptions that arenâ€™t necessarily relevant or correct for AI systems, about things like how well a system is able to memorize.</p><p>As a computer scientist, I didnâ€™t get any training in experimental methodology. Doing experiments on AI systems has become a core part of evaluating systems, and most people who came up through computer science havenâ€™t had that training.</p><p><strong>What do developmental and comparative psychologists know about probing cognition that AI researchers should know too?</strong></p><p><strong>Mitchell:</strong> Thereâ€™s all kinds of experimental methodology that you learn as a student of psychology, especially in fields like developmental and comparative psychology because those are nonverbal agents. You have to really think creatively to figure out ways to probe them. So they have all kinds of methodologies that involve very careful control experiments, and making lots of variations on stimuli to check for robustness. They look carefully at failure modes, why the system [being tested] might fail, since those failures can give more insight into whatâ€™s going on than success.</p><p><strong>Can you give me a concrete example of what these experimental methods look like in developmental or comparative psychology?</strong></p><p><strong>Mitchell:</strong> One classic example is <a href="https://en.wikipedia.org/wiki/Clever_Hans" target="_blank">Clever Hans</a>. There was this horse, Clever Hans, who seemed to be able to do all kinds of arithmetic and counting and other numerical tasks. And the horse would tap out its answer with its hoof. For years, people studied it and said, â€œI think itâ€™s real. Itâ€™s not a hoax.â€ But then <a href="https://en.wikipedia.org/wiki/Oskar_Pfungst" target="_blank">a psychologist</a> came around and said, â€œIâ€™m going to think really hard about whatâ€™s going on and do some control experiments.â€ And his control experiments were: first, put a blindfold on the horse, and second, put a screen between the horse and the question asker. Turns out if the horse couldnâ€™t see the question asker, it couldnâ€™t do the task. What he found was that the horse was actually perceiving very subtle facial expression cues in the asker to know when to stop tapping. So itâ€™s important to come up with alternative explanations for whatâ€™s going on. To be skeptical not only of other peopleâ€™s research, but maybe even of your own research, your own favorite hypothesis. I donâ€™t think that happens enough in AI.</p><p><strong>Do you have any case studies from research on babies?</strong></p><p><strong>Mitchell:</strong> I have one case study where babies were <a href="https://www.nature.com/articles/nature06288.epdf?sharing_token=NaXcdNfjn_ktn5ri3QG45tRgN0jAjWel9jnR3ZoTv0O8aD3KLw-45u9i6UpdAHsxWqm0w4l5z_rcCGu3oMzrydMb---fq01I0LBO5y_TRS_vrnUazfwqqCeiiPTOnsikTBil18Nzkcus3pGuA01UVQr-9DWge0nHwMu5X8gIJgjH90R2IqNR_-8WhIWaEnYnrp4uNLHP54XXfeAQ8FDjt23U-hFVTZg4liviDOMhx5rlatVpfpWvuJ20bMzUGaM8lAw7Q5ccGolZw_BrVmTM6frTBEOkijW7qIckKcTNQdT8gIkRtC2pNc_WiQDIENEvqe2L2Ko_9sZIOkJNoC3Qlg%3D%3D&amp;tracking_referrer=www.smithsonianmag.com" target="_blank">claimed to have an innate moral sense</a>. The experiment showed them videos where there was a cartoon character trying to climb up a hill. In one case there was another character that helped them go up the hill, and in the other case there was a character that pushed them down the hill. So there was the helper and the hinderer. And the babies were assessed as to which character they liked betterâ€”and they had a couple of ways of doing thatâ€”and overwhelmingly they liked the helper character better. [Editor's note: The babies were 6 to 10 months old, and assessment techniques included seeing whether the babies reached for the helper or the hinderer.]</p><p>But another research group looked very carefully at these videos and found that in all of the helper videos, the climber who was being helped was excited to get to the top of the hill and bounced up and down. And so they said, â€œWell, what if in the hinderer case we have the climber bounce up and down at the bottom of the hill?â€ And that <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042698" target="_blank">completely turned around the results</a>. The babies always chose the one that bounced.</p><p>Again, coming up with alternatives, even if you have your favorite hypothesis, is the way that we do science. One thing that Iâ€™m always a little shocked by in AI is that people use the word skeptic as a negative: â€œYouâ€™re an LLM skeptic.â€ But our job is to be skeptics, and that should be a compliment.</p><h2>Importance of Replication in AI Studies</h2><p><strong>Both those examples illustrate the theme of looking for counter explanations. Are there other big lessons that you think AI researchers should draw from psychology?</strong></p><p><strong>Mitchell:</strong> Well, in science in general the idea of replicating experiments is really important, and also building on other peopleâ€™s work. But thatâ€™s sadly a little bit frowned on in the AI world. If you submit a paper to NeurIPS, for example, where you replicated someoneâ€™s work and then you do some incremental thing to understand it, the reviewers will say, â€œThis lacks novelty and itâ€™s incremental.â€ Thatâ€™s the kiss of death for your paper. I feel like that should be appreciated more because thatâ€™s the way that good science gets done.</p><p><strong>Going back to measuring cognitive capabilities of AI, thereâ€™s lots of talk about how we can </strong><a href="https://spectrum.ieee.org/agi-benchmark" target="_self"><strong>measure progress towards AGI</strong></a><strong>. Is that a whole other batch of questions?</strong></p><p><strong>Mitchell:</strong> Well, the term AGI is a little bit nebulous. People define it in different ways. I think itâ€™s hard to measure progress for something thatâ€™s not that well defined. And our conception of it keeps changing, partially in response to things that happen in AI. In the old days of AI, people would talk about human-level intelligence and robots being able to do all the physical things that humans do. But people have looked at robotics and said, â€œWell, okay, itâ€™s not going to get there soon. Letâ€™s just talk about what people call the cognitive side of intelligence,â€ which I donâ€™t think is really so separable. So I am a bit of an AGI skeptic, if you will, in the best way.</p>